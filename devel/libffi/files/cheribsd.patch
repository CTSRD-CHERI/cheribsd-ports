diff --git .clang-format .clang-format
new file mode 100644
index 0000000..3fbf3b4
--- /dev/null
+++ .clang-format
@@ -0,0 +1,3 @@
+BasedOnStyle: GNU
+UseTab: ForContinuationAndIndentation
+SortIncludes: Never
diff --git include/ffi_common.h include/ffi_common.h
index 2bd31b0..87f0108 100644
--- include/ffi_common.h
+++ include/ffi_common.h
@@ -95,10 +95,23 @@ void ffi_type_test(ffi_type *a, char *file, int line);
 #define FFI_ASSERT_VALID_TYPE(x)
 #endif
 
+#ifndef __has_builtin
+#define __has_builtin(...) 0
+#endif
+
+#include <stdint.h>
 /* v cast to size_t and aligned up to a multiple of a */
-#define FFI_ALIGN(v, a)  (((((size_t) (v))-1) | ((a)-1))+1)
+#if __has_builtin(__builtin_align_up)
+#define FFI_ALIGN(v, a)  __builtin_align_up(v, a)
+#else
+#define FFI_ALIGN(v, a)  (((((uintptr_t) (v))-1) | ((a)-1))+1)
+#endif
 /* v cast to size_t and aligned down to a multiple of a */
-#define FFI_ALIGN_DOWN(v, a) (((size_t) (v)) & -a)
+#if __has_builtin(__builtin_align_down)
+#define FFI_ALIGN_DOWN(v, a)  __builtin_align_down(v, a)
+#else
+#define FFI_ALIGN_DOWN(v, a) (((uintptr_t) (v)) & -a)
+#endif
 
 /* Perform machine dependent cif processing */
 ffi_status ffi_prep_cif_machdep(ffi_cif *cif);
diff --git src/aarch64/ffi.c src/aarch64/ffi.c
index 6544ac0..e2e94e6 100644
--- src/aarch64/ffi.c
+++ src/aarch64/ffi.c
@@ -20,6 +20,7 @@ TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
 SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.  */
 
 #if defined(__aarch64__) || defined(__arm64__)|| defined (_M_ARM64)
+#include <stdbool.h>
 #include <stdio.h>
 #include <stdlib.h>
 #include <stdint.h>
@@ -54,10 +55,28 @@ struct _v
   union _d d[2] __attribute__((aligned(16)));
 };
 
+#ifdef __CHERI_PURE_CAPABILITY__
+typedef uintptr_t XREG;
+#else
+typedef uint64_t XREG;
+#endif
+
 struct call_context
 {
   struct _v v[N_V_ARG_REG];
-  UINT64 x[N_X_ARG_REG];
+  XREG x[N_X_ARG_REG];
+#ifdef __CHERI_PURE_CAPABILITY__
+  XREG x9;
+#endif
+};
+
+struct call_frame
+{
+  XREG lr;
+  XREG fp;
+  XREG rvalue;
+  XREG flags;
+  XREG sp;
 };
 
 #if FFI_EXEC_TRAMPOLINE_TABLE
@@ -318,7 +337,7 @@ allocate_and_copy_struct_to_stack (struct arg_state *state, void *stack,
 }
 
 static ffi_arg
-extend_integer_type (void *source, int type)
+extend_integer_type (uintptr_t *source, int type)
 {
   switch (type)
     {
@@ -377,6 +396,44 @@ extend_integer_type (void *source, int type)
     }
 }
 
+static inline bool
+can_pass_aggregate_in_xregs (ffi_type *ty, size_t *num_xregs,
+			     size_t *num_cheri_caps)
+{
+  FFI_ASSERT (ty->elements != NULL);
+  if (ty->size > 2 * X_REG_SIZE)
+    return false;
+  *num_xregs = 0;
+  *num_cheri_caps = 0;
+#ifdef __CHERI_PURE_CAPABILITY__
+  /* For CHERI up to 32 bytes can be okay if we have either 2 caps, or one
+   * capability + (multiple) integers < 8 bytes. */
+  for (int i = 0; ty->elements[i]; i++)
+    {
+      if (ty->elements[i]->type == FFI_TYPE_STRUCT)
+	abort (); /* TODO: should recurse into structs to flatten them. */
+      else if (ty->elements[i]->type == FFI_TYPE_POINTER)
+	(*num_cheri_caps)++;
+      else if (ty->elements[i]->size > 8)
+	return false; /* can't pass cap+int128 in registers */
+    }
+  FFI_ASSERT (*num_cheri_caps <= 2);
+  /* If there are no capabilities, we can only return 16 bytes in xregs. */
+  if (*num_cheri_caps == 0 && ty->size > 16)
+    return false;
+#endif
+  *num_xregs
+      = *num_cheri_caps + ((ty->size - *num_cheri_caps * X_REG_SIZE) + 7) / 8;
+  FFI_ASSERT (*num_xregs <= 2);
+  return true;
+}
+
+#ifdef __CHERI_PURE_CAPABILITY__
+#define PTR_CONSTR "C"
+#else
+#define PTR_CONSTR "r"
+#endif
+
 #if defined(_MSC_VER)
 void extend_hfa_type (void *dest, void *src, int h);
 #else
@@ -387,7 +444,11 @@ extend_hfa_type (void *dest, void *src, int h)
   void *x0;
 
   asm volatile (
+#ifdef __CHERI_PURE_CAPABILITY__
+	"adr	%0, 0f+1\n" /* +1 is needed to stay in C64 mode */
+#else
 	"adr	%0, 0f\n"
+#endif
 "	add	%0, %0, %1\n"
 "	br	%0\n"
 "0:	ldp	s16, s17, [%3]\n"	/* S4 */
@@ -429,8 +490,8 @@ extend_hfa_type (void *dest, void *src, int h)
 "3:	str	q18, [%2, #32]\n"
 "2:	str	q17, [%2, #16]\n"
 "1:	str	q16, [%2]"
-    : "=&r"(x0)
-    : "r"(f * 12), "r"(dest), "r"(src)
+    : "=&" PTR_CONSTR (x0)
+    : "r"(f * 12), PTR_CONSTR(dest), PTR_CONSTR(src)
     : "memory", "v16", "v17", "v18", "v19");
 }
 #endif
@@ -456,19 +517,19 @@ compress_hfa_type (void *dest, void *reg, int h)
     case AARCH64_RET_S2:
       asm ("ldp q16, q17, [%1]\n\t"
 	   "st2 { v16.s, v17.s }[0], [%0]"
-	   : : "r"(dest), "r"(reg) : "memory", "v16", "v17");
+	   : : PTR_CONSTR(dest), PTR_CONSTR(reg) : "memory", "v16", "v17");
       break;
     case AARCH64_RET_S3:
       asm ("ldp q16, q17, [%1]\n\t"
 	   "ldr q18, [%1, #32]\n\t"
 	   "st3 { v16.s, v17.s, v18.s }[0], [%0]"
-	   : : "r"(dest), "r"(reg) : "memory", "v16", "v17", "v18");
+	   : : PTR_CONSTR(dest), PTR_CONSTR(reg) : "memory", "v16", "v17", "v18");
       break;
     case AARCH64_RET_S4:
       asm ("ldp q16, q17, [%1]\n\t"
 	   "ldp q18, q19, [%1, #32]\n\t"
 	   "st4 { v16.s, v17.s, v18.s, v19.s }[0], [%0]"
-	   : : "r"(dest), "r"(reg) : "memory", "v16", "v17", "v18", "v19");
+	   : : PTR_CONSTR(dest), PTR_CONSTR(reg) : "memory", "v16", "v17", "v18", "v19");
       break;
 
     case AARCH64_RET_D1:
@@ -484,19 +545,19 @@ compress_hfa_type (void *dest, void *reg, int h)
     case AARCH64_RET_D2:
       asm ("ldp q16, q17, [%1]\n\t"
 	   "st2 { v16.d, v17.d }[0], [%0]"
-	   : : "r"(dest), "r"(reg) : "memory", "v16", "v17");
+	   : : PTR_CONSTR(dest), PTR_CONSTR(reg) : "memory", "v16", "v17");
       break;
     case AARCH64_RET_D3:
       asm ("ldp q16, q17, [%1]\n\t"
 	   "ldr q18, [%1, #32]\n\t"
 	   "st3 { v16.d, v17.d, v18.d }[0], [%0]"
-	   : : "r"(dest), "r"(reg) : "memory", "v16", "v17", "v18");
+	   : : PTR_CONSTR(dest), PTR_CONSTR(reg) : "memory", "v16", "v17", "v18");
       break;
     case AARCH64_RET_D4:
       asm ("ldp q16, q17, [%1]\n\t"
 	   "ldp q18, q19, [%1, #32]\n\t"
 	   "st4 { v16.d, v17.d, v18.d, v19.d }[0], [%0]"
-	   : : "r"(dest), "r"(reg) : "memory", "v16", "v17", "v18", "v19");
+	   : : PTR_CONSTR(dest), PTR_CONSTR(reg) : "memory", "v16", "v17", "v18", "v19");
       break;
 
     default:
@@ -560,7 +621,7 @@ ffi_prep_cif_machdep (ffi_cif *cif)
       flags = AARCH64_RET_INT64;
       break;
     case FFI_TYPE_POINTER:
-      flags = (sizeof(void *) == 4 ? AARCH64_RET_UINT32 : AARCH64_RET_INT64);
+      flags = (sizeof(void *) == 4 ? AARCH64_RET_UINT32 : AARCH64_RET_POINTER);
       break;
 
     case FFI_TYPE_FLOAT:
@@ -638,12 +699,32 @@ ffi_call_int (ffi_cif *cif, void (*fn)(void), void *orig_rvalue,
 	      void **avalue, void *closure)
 {
   struct call_context *context;
-  void *stack, *frame, *rvalue;
+  struct call_frame *frame;
+  void *stack, *rvalue;
   struct arg_state state;
-  size_t stack_bytes, rtype_size, rsize;
+  size_t stack_bytes, rtype_size, rsize, total_length;
+  size_t context_length, context_offset, frame_length, frame_offset;
+  size_t rvalue_length, rvalue_offset, stack_length, stack_offset;
   int i, nargs, flags, isvariadic = 0;
+  char *cursor;
   ffi_type *rtype;
 
+#ifdef __CHERI_PURE_CAPABILITY__
+#define SUBOBJECT_ALIGNMENT(length)					\
+	~__builtin_cheri_representable_alignment_mask(length) + 1
+#define SUBOBJECT_LENGTH(length)						\
+	__builtin_cheri_round_representable_length(length)
+#else
+#define SUBOBJECT_ALIGNMENT(length) 1
+#define SUBOBJECT_LENGTH(length) length
+#endif
+#define SUBOBJECT_LAYOUT(length, subobj_length, subobj_offset, size, type) \
+	subobj_length = SUBOBJECT_LENGTH(size);				\
+	length = __align_up(length, _Alignof(type));			\
+	length = __align_up(length, SUBOBJECT_ALIGNMENT(subobj_length)); \
+	subobj_offset = length;						\
+	length += subobj_length;
+
   flags = cif->flags;
   rtype = cif->rtype;
   rtype_size = rtype->size;
@@ -669,19 +750,58 @@ ffi_call_int (ffi_cif *cif, void (*fn)(void), void *orig_rvalue,
   else if (flags & AARCH64_RET_NEED_COPY)
     rsize = 16;
 
-  /* Allocate consectutive stack for everything we'll need.
-     The frame uses 40 bytes for: lr, fp, rvalue, flags, sp */
-  context = alloca (sizeof(struct call_context) + stack_bytes + 40 + rsize);
-  stack = context + 1;
-  frame = (void*)((uintptr_t)stack + (uintptr_t)stack_bytes);
-  rvalue = (rsize ? (void*)((uintptr_t)frame + 40) : orig_rvalue);
+  /*
+   * Compute sub-objects' layouts, i.e. sizes and offsets.
+   *
+   * On CHERI-extended architectures, the allocation is appropriately padded to
+   * use precise bounds for sub-objects, including the stack and on-stack
+   * function arguments referenced by the c9 capability register.
+   */
+  total_length = 0;
+  SUBOBJECT_LAYOUT(total_length, context_length, context_offset,
+    sizeof(*context), typeof(*context));
+  _Static_assert(sizeof(*context) == CALL_CONTEXT_SIZE, "");
+  SUBOBJECT_LAYOUT(total_length, stack_length, stack_offset,
+    stack_bytes, max_align_t);
+  SUBOBJECT_LAYOUT(total_length, frame_length, frame_offset, sizeof(*frame),
+    typeof(*frame));
+  _Static_assert(sizeof(*frame) == CALL_FRAME_SIZE, "");
+  SUBOBJECT_LAYOUT(total_length, rvalue_length, rvalue_offset, rsize,
+    max_align_t);
+
+  /*
+   * Allocate consectutive stack for everything we'll need.
+   * The frame uses 40/80 bytes for: lr, fp, rvalue, flags, sp.
+   */
+  context = alloca (total_length);
+
+  cursor = (char *)context;
+  context = (struct call_context *)(cursor + context_offset);
+  stack = (void *)(cursor + stack_offset);
+  frame = (struct call_frame *)(cursor + frame_offset);
+#ifdef __CHERI_PURE_CAPABILITY__
+  context = __builtin_cheri_bounds_set_exact(context, context_length);
+  stack = __builtin_cheri_bounds_set_exact(stack, stack_length);
+  frame = __builtin_cheri_bounds_set_exact(frame, frame_length);
+#endif
+  /* Initialize the frame with 0xaa to detect errors. */
+  memset(context, 0xaa, context_length);
+  memset(frame, 0xaa, frame_length);
+  if (rsize) {
+    rvalue = (void *)(cursor + rvalue_offset);
+#ifdef __CHERI_PURE_CAPABILITY__
+    rvalue = __builtin_cheri_bounds_set_exact(rvalue, rvalue_length);
+#endif
+  } else {
+    rvalue = orig_rvalue;
+  }
 
   arg_init (&state, stack_bytes);
   for (i = 0, nargs = cif->nargs; i < nargs; i++)
     {
       ffi_type *ty = cif->arg_types[i];
       size_t s = ty->size;
-      void *a = avalue[i];
+      uintptr_t *a = avalue[i];
       int h, t;
       void *dest;
 
@@ -719,7 +839,14 @@ ffi_call_int (ffi_cif *cif, void (*fn)(void), void *orig_rvalue,
 #ifdef __APPLE__
 		memcpy(d, a, s);
 #else
-		*(ffi_arg *)d = ext;
+		/* Integers are extended to uint64_t, but for Morello pointers
+		 * are be 16 bytes, so we have to use uintptr_t/ffi_arg. */
+		if (t == FFI_TYPE_POINTER) {
+		  FFI_ASSERT(s == sizeof(void*));
+		  *(uintptr_t *)d = ext;
+		} else {
+		  *(uint64_t *)d = (uint64_t)ext;
+		}
 #endif
 	      }
 	  }
@@ -731,6 +858,9 @@ ffi_call_int (ffi_cif *cif, void (*fn)(void), void *orig_rvalue,
 	case FFI_TYPE_STRUCT:
 	case FFI_TYPE_COMPLEX:
 	  {
+	    size_t num_capabilities = 0;
+	    size_t num_xregs = 0;
+
 	    h = is_vfp_type (ty);
 	    if (h)
 	      {
@@ -760,7 +890,8 @@ ffi_call_int (ffi_cif *cif, void (*fn)(void), void *orig_rvalue,
                 dest = allocate_to_stack (&state, stack, ty->alignment, s);
               }
 	      }
-	    else if (s > 16)
+	    else if (!can_pass_aggregate_in_xregs (ty, &num_xregs,
+						   &num_capabilities))
 	      {
 		/* If the argument is a composite type that is larger than 16
 		   bytes, then the argument is copied to memory, and
@@ -768,22 +899,43 @@ ffi_call_int (ffi_cif *cif, void (*fn)(void), void *orig_rvalue,
 		dest = allocate_and_copy_struct_to_stack (&state, stack,
 							  ty->alignment, s,
 							  avalue[i]);
-		a = &dest;
+		a = (uintptr_t *)&dest;
 		t = FFI_TYPE_POINTER;
 		s = sizeof (void *);
 		goto do_pointer;
 	      }
 	    else
 	      {
-		size_t n = (s + 7) / 8;
-		if (state.ngrn + n <= N_X_ARG_REG)
+		if (state.ngrn + num_xregs <= N_X_ARG_REG)
 		  {
+		    /* If the struct type does not contain capabilities, we
+		     * have to pass two separate 8-byte chunks since context->x
+		     * contains 16-byte registers */
+		    if (num_capabilities == 0)
+		      {
+			FFI_ASSERT (num_xregs == (s + 7) / 8);
+			for (int offset = 0; offset < s; offset += 8)
+			  {
+			    uint64_t tmp = 0;
+			    memcpy (&tmp, (uint8_t *)a + offset,
+				    s - offset > 8 ? 8 : s - offset);
+			    context->x[state.ngrn] = tmp;
+			    state.ngrn++;
+			  }
+			break; /* copy already completed */
+		      }
 		    /* If the argument is a composite type and the size in
 		       double-words is not more than the number of available
 		       X registers, then the argument is copied into
-		       consecutive X registers.  */
-		    dest = &context->x[state.ngrn];
-                    state.ngrn += (unsigned int)n;
+		       consecutive X registers.
+		       NB: If the struct contains capabilities, the layout is
+		       padded appropriately, so we can do a simple memcpy().
+		       */
+		    else
+		      {
+			dest = &context->x[state.ngrn];
+			state.ngrn += num_xregs;
+		      }
 		  }
 		else
 		  {
@@ -813,10 +965,38 @@ ffi_call_int (ffi_cif *cif, void (*fn)(void), void *orig_rvalue,
 #endif
     }
 
+#ifdef __CHERI_PURE_CAPABILITY__
+  /*
+   * Narrow bounds of c9 to on-stack arguments excluding argument structure
+   * objects passed by value in the function call which were replaced by
+   * pointers to the objects in the on-stack arguments.
+   *
+   * We can use precise bounds for the stack capability because it is already
+   * aligned to a larger size than its effective bounds.
+   */
+  if (state.nsaa > 0) {
+    FFI_ASSERT(SUBOBJECT_LENGTH(state.nsaa) <= stack_length);
+    FFI_ASSERT(stack == __align_up(stack, SUBOBJECT_ALIGNMENT(state.nsaa)));
+    context->x9 = (XREG)__builtin_cheri_bounds_set_exact(stack,
+      SUBOBJECT_LENGTH(state.nsaa));
+  } else {
+    /*
+     * CheriABI requires c9 to be set to NULL if there are no on-stack variadic
+     * arguments. In other cases, we want to clear 0xaa to indicate c9 was
+     * correctly handled.
+     */
+    context->x9 = (XREG)NULL;
+  }
+#endif
+
   ffi_call_SYSV (context, frame, fn, rvalue, flags, closure);
 
   if (flags & AARCH64_RET_NEED_COPY)
     memcpy (orig_rvalue, rvalue, rtype_size);
+
+#undef SUBOBJECT_LENGTH
+#undef SUBOBJECT_ALIGNMENT
+#undef SUBOBJECT_LAYOUT
 }
 
 void
@@ -895,9 +1075,10 @@ ffi_prep_closure_loc (ffi_closure *closure,
   /* Initialize the dynamic trampoline. */
   memcpy (tramp, trampoline, sizeof(trampoline));
 
-  *(UINT64 *)(tramp + 16) = (uintptr_t)start;
+  *(XREG *)(tramp + 16) = (uintptr_t)start;
 
   ffi_clear_cache(tramp, tramp + FFI_TRAMPOLINE_SIZE);
+  _Static_assert(FFI_TRAMPOLINE_SIZE == 16 + sizeof(XREG), "");
 
   /* Also flush the cache for code mapping.  */
 # ifdef _WIN32
diff --git src/aarch64/ffitarget.h src/aarch64/ffitarget.h
index d5622e1..91585c3 100644
--- src/aarch64/ffitarget.h
+++ src/aarch64/ffitarget.h
@@ -27,7 +27,11 @@ SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.  */
 #endif
 
 #ifndef LIBFFI_ASM
-#ifdef __ILP32__
+#if defined(__CHERI_PURE_CAPABILITY__)
+#define FFI_SIZEOF_ARG __SIZEOF_POINTER__
+typedef __UINTPTR_TYPE__ ffi_arg;
+typedef __INTPTR_TYPE__ ffi_sarg;
+#elif defined(__ILP32__)
 #define FFI_SIZEOF_ARG 8
 #define FFI_SIZEOF_JAVA_RAW  4
 typedef unsigned long long ffi_arg;
@@ -57,7 +61,12 @@ typedef enum ffi_abi
 
 /* ---- Definitions for closures ----------------------------------------- */
 
+#if defined(__CHERI_PURE_CAPABILITY__)
+/* Not implemented yet for purecap. */
+#define FFI_CLOSURES 0
+#else
 #define FFI_CLOSURES 1
+#endif
 #define FFI_NATIVE_RAW_API 0
 
 #if defined (FFI_EXEC_TRAMPOLINE_TABLE) && FFI_EXEC_TRAMPOLINE_TABLE
@@ -69,8 +78,12 @@ typedef enum ffi_abi
 #error "No trampoline table implementation"
 #endif
 
+#else
+#ifdef __CHERI_PURE_CAPABILITY__
+#define FFI_TRAMPOLINE_SIZE 32
 #else
 #define FFI_TRAMPOLINE_SIZE 24
+#endif
 #define FFI_TRAMPOLINE_CLOSURE_OFFSET FFI_TRAMPOLINE_SIZE
 #endif
 
diff --git src/aarch64/internal.h src/aarch64/internal.h
index b5d102b..e8c4df6 100644
--- src/aarch64/internal.h
+++ src/aarch64/internal.h
@@ -22,7 +22,7 @@ SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.  */
 #define AARCH64_RET_INT64	1
 #define AARCH64_RET_INT128	2
 
-#define AARCH64_RET_UNUSED3	3
+#define AARCH64_RET_POINTER	3
 #define AARCH64_RET_UNUSED4	4
 #define AARCH64_RET_UNUSED5	5
 #define AARCH64_RET_UNUSED6	6
@@ -64,8 +64,19 @@ SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.  */
 #define AARCH64_FLAG_VARARG	(1 << 8)
 
 #define N_X_ARG_REG		8
+#ifdef __CHERI_PURE_CAPABILITY__
+#define X_REG_SIZE 16
+#else
+#define X_REG_SIZE 8
+#endif
 #define N_V_ARG_REG		8
-#define CALL_CONTEXT_SIZE	(N_V_ARG_REG * 16 + N_X_ARG_REG * 8)
+#ifdef __CHERI_PURE_CAPABILITY__
+#define CALL_CONTEXT_SIZE						\
+	(N_V_ARG_REG * 16 + N_X_ARG_REG * X_REG_SIZE + X_REG_SIZE)
+#else
+#define CALL_CONTEXT_SIZE	(N_V_ARG_REG * 16 + N_X_ARG_REG * X_REG_SIZE)
+#endif
+#define CALL_FRAME_SIZE		(5 * X_REG_SIZE)
 
 #if defined(FFI_EXEC_STATIC_TRAMP)
 /*
@@ -82,6 +93,9 @@ SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.  */
 #ifdef LIBFFI_ASM
 
 #ifdef HAVE_PTRAUTH
+#ifdef __CHERI_PURE_CAPABILITY__
+#error "HAVE_PTRAUTH is not compatible"
+#endif
 #define SIGN_LR pacibsp
 #define SIGN_LR_WITH_REG(x) pacib lr, x
 #define AUTH_LR_AND_RET retab
@@ -91,7 +105,11 @@ SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.  */
 #else
 #define SIGN_LR
 #define SIGN_LR_WITH_REG(x)
+#ifdef __CHERI_PURE_CAPABILITY__
+#define AUTH_LR_AND_RET ret c30
+#else
 #define AUTH_LR_AND_RET ret
+#endif
 #define AUTH_LR_WITH_REG(x)
 #define BRANCH_AND_LINK_TO_REG blr
 #define BRANCH_TO_REG br
diff --git src/aarch64/sysv.S src/aarch64/sysv.S
index eeaf3f8..becc717 100644
--- src/aarch64/sysv.S
+++ src/aarch64/sysv.S
@@ -46,40 +46,71 @@ SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.  */
 # define BE(X)	0
 #endif
 
+#ifdef __CHERI_PURE_CAPABILITY__
+/* XXX: defining sp to csp is a rather ugly hack to avoid more changes. */
+#define sp              csp
+#define PTR_REG(n)      c##n
+#define INT_REG(n)      x##n
+#define GP_REG(n)       c##n
+#else
 #ifdef __ILP32__
 #define PTR_REG(n)      w##n
 #else
 #define PTR_REG(n)      x##n
 #endif
+#define GP_REG(n)       x##n
+#define INT_REG(n)      x##n
+#endif
 
+#ifdef __CHERI_PURE_CAPABILITY__
+#define PTR_SIZE	16
+#define GP_REG_SIZE	16
+#else
 #ifdef __ILP32__
 #define PTR_SIZE	4
 #else
 #define PTR_SIZE	8
+#endif
+#define GP_REG_SIZE	8
+#endif
+
+#ifdef __ARM_MORELLO_PURECAP_BENCHMARK_ABI
+#define BR_PTR_REG(n)	INT_REG(n)
+#define RETURN							\
+	and	x30, x30, #~1;					\
+	ret	x30
+#else
+#define BR_PTR_REG(n)	PTR_REG(n)
+#define RETURN							\
+	ret
 #endif
 
 	.text
 	.align 4
 
 /* ffi_call_SYSV
-   extern void ffi_call_SYSV (void *stack, void *frame,
+   extern void ffi_call_SYSV (struct call_context *context, void *frame,
 			      void (*fn)(void), void *rvalue,
 			      int flags, void *closure);
 
    Therefore on entry we have:
 
-   x0 stack
+   x0 context
    x1 frame
    x2 fn
    x3 rvalue
    x4 flags
    x5 closure
 */
-
+	.globl	CNAME(ffi_call_SYSV)
+	FFI_HIDDEN(CNAME(ffi_call_SYSV))
+#ifdef __ELF__
+	.type	CNAME(ffi_call_SYSV), #function
+#endif
 	cfi_startproc
 CNAME(ffi_call_SYSV):
 	/* Sign the lr with x1 since that is where it will be stored */
-	SIGN_LR_WITH_REG(x1)
+	SIGN_LR_WITH_REG(PTR_REG(1))
 
 	/* Use a stack frame allocated by our caller.  */
 #if defined(HAVE_PTRAUTH) && defined(__APPLE__)
@@ -87,25 +118,29 @@ CNAME(ffi_call_SYSV):
 	 * used to sign the lr.  In order to allow unwinding through this
 	 * function it is necessary to point the cfa at the signing register.
 	 */
-	cfi_def_cfa(x1, 0);
+	cfi_def_cfa(GP_REG(1), 0);
+#else
+	cfi_def_cfa(GP_REG(1), CALL_FRAME_SIZE);
+#endif
+	stp	GP_REG(29), GP_REG(30), [PTR_REG(1)]
+	mov	GP_REG(9), sp
+	str	GP_REG(9), [PTR_REG(1), #4*GP_REG_SIZE]
+	mov	GP_REG(29), GP_REG(1)
+#ifdef __CHERI_PURE_CAPABILITY__
+	scvalue	csp, csp, x0	/* restore stack bounds on context */
 #else
-	cfi_def_cfa(x1, 40);
-#endif
-	stp	x29, x30, [x1]
-	mov	x9, sp
-	str	x9, [x1, #32]
-	mov	x29, x1
-	mov	sp, x0
-	cfi_def_cfa_register(x29)
-	cfi_rel_offset (x29, 0)
-	cfi_rel_offset (x30, 8)
-
-	mov	x9, x2			/* save fn */
-	mov	x8, x3			/* install structure return */
+	mov	sp, GP_REG(0)
+#endif
+	cfi_def_cfa_register(GP_REG(29))
+	cfi_rel_offset (GP_REG(29), 0)
+	cfi_rel_offset (GP_REG(30), GP_REG_SIZE)
+
+	mov	GP_REG(10), GP_REG(2)			/* save fn */
+	mov	GP_REG(8), GP_REG(3)			/* install structure return */
 #ifdef FFI_GO_CLOSURES
-	mov	x18, x5			/* install static chain */
+	mov	GP_REG(18), GP_REG(5)			/* install static chain */
 #endif
-	stp	x3, x4, [x29, #16]	/* save rvalue and flags */
+	stp	GP_REG(3), GP_REG(4), [PTR_REG(29), #2*GP_REG_SIZE]	/* save rvalue and flags */
 
 	/* Load the vector argument passing registers, if necessary.  */
 	tbz	w4, #AARCH64_FLAG_ARG_V_BIT, 1f
@@ -116,30 +151,35 @@ CNAME(ffi_call_SYSV):
 1:
 	/* Load the core argument passing registers, including
 	   the structure return pointer.  */
-	ldp     x0, x1, [sp, #16*N_V_ARG_REG + 0]
-	ldp     x2, x3, [sp, #16*N_V_ARG_REG + 16]
-	ldp     x4, x5, [sp, #16*N_V_ARG_REG + 32]
-	ldp     x6, x7, [sp, #16*N_V_ARG_REG + 48]
+	ldp     GP_REG(0), GP_REG(1), [sp, #16*N_V_ARG_REG + 0*GP_REG_SIZE]
+	ldp     GP_REG(2), GP_REG(3), [sp, #16*N_V_ARG_REG + 2*GP_REG_SIZE]
+	ldp     GP_REG(4), GP_REG(5), [sp, #16*N_V_ARG_REG + 4*GP_REG_SIZE]
+	ldp     GP_REG(6), GP_REG(7), [sp, #16*N_V_ARG_REG + 6*GP_REG_SIZE]
+#ifdef __CHERI_PURE_CAPABILITY__
+	ldr     GP_REG(9), [sp, #16*N_V_ARG_REG + 8*GP_REG_SIZE]
+#endif
 
 	/* Deallocate the context, leaving the stacked arguments.  */
 	add	sp, sp, #CALL_CONTEXT_SIZE
 
-	BRANCH_AND_LINK_TO_REG     x9			/* call fn */
+	BRANCH_AND_LINK_TO_REG     BR_PTR_REG(10)		/* call fn */
 
-	ldp	x3, x4, [x29, #16]	/* reload rvalue and flags */
+	ldp	GP_REG(3), GP_REG(4), [PTR_REG(29), #2*GP_REG_SIZE]	/* reload rvalue and flags */
 
 	/* Partially deconstruct the stack frame.  */
-	ldr	x9, [x29, #32]
-	mov	sp, x9
+	ldr	GP_REG(9), [PTR_REG(29), #4*GP_REG_SIZE]
+	mov	sp, GP_REG(9)
 	cfi_def_cfa_register (sp)
-	mov	x2, x29			/* Preserve for auth */
-	ldp     x29, x30, [x29]
+	cfi_rel_offset (GP_REG(29), 0)
+	cfi_rel_offset (GP_REG(30), GP_REG_SIZE)
+	mov	GP_REG(2), GP_REG(29)			/* Preserve for auth */
+	ldp     GP_REG(29), GP_REG(30), [PTR_REG(29)]
 
 	/* Save the return value as directed.  */
-	adr	x5, 0f
+	adr	GP_REG(5), 0f
 	and	w4, w4, #AARCH64_RET_MASK
-	add	x5, x5, x4, lsl #3
-	br	x5
+	add	PTR_REG(5), PTR_REG(5), INT_REG(4), lsl #3
+	br	INT_REG(5)
 
 	/* Note that each table entry is 2 insns, and thus 8 bytes.
 	   For integer data, note that we're storing into ffi_arg
@@ -148,11 +188,11 @@ CNAME(ffi_call_SYSV):
 	.align	4
 0:	b 99f				/* VOID */
 	nop
-1:	str	x0, [x3]		/* INT64 */
+1:	str	x0, [PTR_REG(3)]		/* INT64 */
 	b 99f
-2:	stp	x0, x1, [x3]		/* INT128 */
+2:	stp	x0, x1, [PTR_REG(3)]		/* INT128 */
 	b 99f
-3:	brk	#1000			/* UNUSED */
+3:	str	PTR_REG(0), [PTR_REG(3)]	/* POINTER */
 	b 99f
 4:	brk	#1000			/* UNUSED */
 	b 99f
@@ -162,66 +202,63 @@ CNAME(ffi_call_SYSV):
 	b 99f
 7:	brk	#1000			/* UNUSED */
 	b 99f
-8:	st4	{ v0.s, v1.s, v2.s, v3.s }[0], [x3]	/* S4 */
+8:	st4	{ v0.s, v1.s, v2.s, v3.s }[0], [PTR_REG(3)]	/* S4 */
 	b 99f
-9:	st3	{ v0.s, v1.s, v2.s }[0], [x3]	/* S3 */
+9:	st3	{ v0.s, v1.s, v2.s }[0], [PTR_REG(3)]	/* S3 */
 	b 99f
-10:	stp	s0, s1, [x3]		/* S2 */
+10:	stp	s0, s1, [PTR_REG(3)]		/* S2 */
 	b 99f
-11:	str	s0, [x3]		/* S1 */
+11:	str	s0, [PTR_REG(3)]		/* S1 */
 	b 99f
-12:	st4	{ v0.d, v1.d, v2.d, v3.d }[0], [x3]	/* D4 */
+12:	st4	{ v0.d, v1.d, v2.d, v3.d }[0], [PTR_REG(3)]	/* D4 */
 	b 99f
-13:	st3	{ v0.d, v1.d, v2.d }[0], [x3]	/* D3 */
+13:	st3	{ v0.d, v1.d, v2.d }[0], [PTR_REG(3)]	/* D3 */
 	b 99f
-14:	stp	d0, d1, [x3]		/* D2 */
+14:	stp	d0, d1, [PTR_REG(3)]		/* D2 */
 	b 99f
-15:	str	d0, [x3]		/* D1 */
+15:	str	d0, [PTR_REG(3)]		/* D1 */
 	b 99f
-16:	str	q3, [x3, #48]		/* Q4 */
+16:	str	q3, [PTR_REG(3), #48]		/* Q4 */
 	nop
-17:	str	q2, [x3, #32]		/* Q3 */
+17:	str	q2, [PTR_REG(3), #32]		/* Q3 */
 	nop
-18:	stp	q0, q1, [x3]		/* Q2 */
+18:	stp	q0, q1, [PTR_REG(3)]		/* Q2 */
 	b 99f
-19:	str	q0, [x3]		/* Q1 */
+19:	str	q0, [PTR_REG(3)]		/* Q1 */
 	b 99f
 20:	uxtb	w0, w0			/* UINT8 */
-	str	x0, [x3]
+	str	x0, [PTR_REG(3)]
 21:	b 99f				/* reserved */
 	nop
 22:	uxth	w0, w0			/* UINT16 */
-	str	x0, [x3]
+	str	x0, [PTR_REG(3)]
 23:	b 99f				/* reserved */
 	nop
 24:	mov	w0, w0			/* UINT32 */
-	str	x0, [x3]
+	str	x0, [PTR_REG(3)]
 25:	b 99f				/* reserved */
 	nop
 26:	sxtb	x0, w0			/* SINT8 */
-	str	x0, [x3]
+	str	x0, [PTR_REG(3)]
 27:	b 99f				/* reserved */
 	nop
 28:	sxth	x0, w0			/* SINT16 */
-	str	x0, [x3]
+	str	x0, [PTR_REG(3)]
 29:	b 99f				/* reserved */
 	nop
 30:	sxtw	x0, w0			/* SINT32 */
-	str	x0, [x3]
+	str	x0, [PTR_REG(3)]
 31:	b 99f				/* reserved */
 	nop
 
 	/* Return now that result has been populated. */
 99:
-	AUTH_LR_WITH_REG(x2)
-	ret
+	AUTH_LR_WITH_REG(PTR_REG(2))
+	RETURN
 
 	cfi_endproc
 
-	.globl	CNAME(ffi_call_SYSV)
-	FFI_HIDDEN(CNAME(ffi_call_SYSV))
 #ifdef __ELF__
-	.type	CNAME(ffi_call_SYSV), #function
 	.size CNAME(ffi_call_SYSV), .-CNAME(ffi_call_SYSV)
 #endif
 
@@ -244,13 +281,18 @@ CNAME(ffi_call_SYSV):
 #define ffi_closure_SYSV_FS (8*2 + CALL_CONTEXT_SIZE + 64)
 
 	.align 4
+	.globl	CNAME(ffi_closure_SYSV_V)
+	FFI_HIDDEN(CNAME(ffi_closure_SYSV_V))
+#ifdef __ELF__
+	.type	CNAME(ffi_closure_SYSV_V), #function
+#endif
 CNAME(ffi_closure_SYSV_V):
 	cfi_startproc
 	SIGN_LR
-	stp     x29, x30, [sp, #-ffi_closure_SYSV_FS]!
+	stp     GP_REG(29), GP_REG(30), [sp, #-ffi_closure_SYSV_FS]!
 	cfi_adjust_cfa_offset (ffi_closure_SYSV_FS)
-	cfi_rel_offset (x29, 0)
-	cfi_rel_offset (x30, 8)
+	cfi_rel_offset (GP_REG(29), 0)
+	cfi_rel_offset (GP_REG(30), GP_REG_SIZE)
 
 	/* Save the argument passing vector registers.  */
 	stp     q0, q1, [sp, #16 + 0]
@@ -260,58 +302,63 @@ CNAME(ffi_closure_SYSV_V):
 	b	0f
 	cfi_endproc
 
-	.globl	CNAME(ffi_closure_SYSV_V)
-	FFI_HIDDEN(CNAME(ffi_closure_SYSV_V))
 #ifdef __ELF__
-	.type	CNAME(ffi_closure_SYSV_V), #function
 	.size	CNAME(ffi_closure_SYSV_V), . - CNAME(ffi_closure_SYSV_V)
 #endif
 
 	.align	4
+	.globl	CNAME(ffi_closure_SYSV)
+	FFI_HIDDEN(CNAME(ffi_closure_SYSV))
+#ifdef __ELF__
+	.type	CNAME(ffi_closure_SYSV), #function
+#endif
 	cfi_startproc
 CNAME(ffi_closure_SYSV):
 	SIGN_LR
-	stp     x29, x30, [sp, #-ffi_closure_SYSV_FS]!
+	stp     GP_REG(29), GP_REG(30), [sp, #-ffi_closure_SYSV_FS]!
 	cfi_adjust_cfa_offset (ffi_closure_SYSV_FS)
-	cfi_rel_offset (x29, 0)
-	cfi_rel_offset (x30, 8)
+	cfi_rel_offset (GP_REG(29), 0)
+	cfi_rel_offset (GP_REG(30), GP_REG_SIZE)
 0:
-	mov     x29, sp
+	mov     GP_REG(29), sp
 
 	/* Save the argument passing core registers.  */
-	stp     x0, x1, [sp, #16 + 16*N_V_ARG_REG + 0]
-	stp     x2, x3, [sp, #16 + 16*N_V_ARG_REG + 16]
-	stp     x4, x5, [sp, #16 + 16*N_V_ARG_REG + 32]
-	stp     x6, x7, [sp, #16 + 16*N_V_ARG_REG + 48]
+	stp     GP_REG(0), GP_REG(1), [sp, #16 + 16*N_V_ARG_REG + 0*GP_REG_SIZE]
+	stp     GP_REG(2), GP_REG(3), [sp, #16 + 16*N_V_ARG_REG + 2*GP_REG_SIZE]
+	stp     GP_REG(4), GP_REG(5), [sp, #16 + 16*N_V_ARG_REG + 4*GP_REG_SIZE]
+	stp     GP_REG(6), GP_REG(7), [sp, #16 + 16*N_V_ARG_REG + 6*GP_REG_SIZE]
 
 	/* Load ffi_closure_inner arguments.  */
-	ldp	PTR_REG(0), PTR_REG(1), [x17, #FFI_TRAMPOLINE_CLOSURE_OFFSET]	/* load cif, fn */
-	ldr	PTR_REG(2), [x17, #FFI_TRAMPOLINE_CLOSURE_OFFSET+PTR_SIZE*2]	/* load user_data */
+	ldp	PTR_REG(0), PTR_REG(1), [PTR_REG(17), #FFI_TRAMPOLINE_CLOSURE_OFFSET]	/* load cif, fn */
+	ldr	PTR_REG(2), [PTR_REG(17), #FFI_TRAMPOLINE_CLOSURE_OFFSET+PTR_SIZE*2]	/* load user_data */
 #ifdef FFI_GO_CLOSURES
 .Ldo_closure:
 #endif
-	add	x3, sp, #16				/* load context */
-	add	x4, sp, #ffi_closure_SYSV_FS		/* load stack */
-	add	x5, sp, #16+CALL_CONTEXT_SIZE		/* load rvalue */
-	mov	x6, x8					/* load struct_rval */
+	add	PTR_REG(3), sp, #16				/* load context */
+	add	PTR_REG(4), sp, #ffi_closure_SYSV_FS		/* load stack */
+	add	PTR_REG(5), sp, #16+CALL_CONTEXT_SIZE		/* load rvalue */
+	mov	GP_REG(6), GP_REG(8)					/* load struct_rval */
+#ifdef __ELF__
+	.type	CNAME(ffi_closure_SYSV_inner), #function
+#endif
 	bl      CNAME(ffi_closure_SYSV_inner)
 
 	/* Load the return value as directed.  */
-	adr	x1, 0f
+	adr	PTR_REG(1), 0f
 	and	w0, w0, #AARCH64_RET_MASK
-	add	x1, x1, x0, lsl #3
-	add	x3, sp, #16+CALL_CONTEXT_SIZE
-	br	x1
+	add	PTR_REG(1), PTR_REG(1), INT_REG(0), lsl #3
+	add	PTR_REG(3), sp, #16+CALL_CONTEXT_SIZE
+	br	BR_PTR_REG(1)
 
 	/* Note that each table entry is 2 insns, and thus 8 bytes.  */
 	.align	4
 0:	b	99f			/* VOID */
 	nop
-1:	ldr	x0, [x3]		/* INT64 */
+1:	ldr	x0, [PTR_REG(3)]		/* INT64 */
 	b	99f
-2:	ldp	x0, x1, [x3]		/* INT128 */
+2:	ldp	x0, x1, [PTR_REG(3)]		/* INT128 */
 	b	99f
-3:	brk	#1000			/* UNUSED */
+3:	ldr	PTR_REG(0), [PTR_REG(3)]	/* POINTER */
 	nop
 4:	brk	#1000			/* UNUSED */
 	nop
@@ -321,93 +368,94 @@ CNAME(ffi_closure_SYSV):
 	nop
 7:	brk	#1000			/* UNUSED */
 	nop
-8:	ldr	s3, [x3, #12]		/* S4 */
+8:	ldr	s3, [PTR_REG(3), #12]		/* S4 */
 	nop
-9:	ldr	s2, [x3, #8]		/* S3 */
+9:	ldr	s2, [PTR_REG(3), #8]		/* S3 */
 	nop
-10:	ldp	s0, s1, [x3]		/* S2 */
+10:	ldp	s0, s1, [PTR_REG(3)]		/* S2 */
 	b	99f
-11:	ldr	s0, [x3]		/* S1 */
+11:	ldr	s0, [PTR_REG(3)]		/* S1 */
 	b	99f
-12:	ldr	d3, [x3, #24]		/* D4 */
+12:	ldr	d3, [PTR_REG(3), #24]		/* D4 */
 	nop
-13:	ldr	d2, [x3, #16]		/* D3 */
+13:	ldr	d2, [PTR_REG(3), #16]		/* D3 */
 	nop
-14:	ldp	d0, d1, [x3]		/* D2 */
+14:	ldp	d0, d1, [PTR_REG(3)]		/* D2 */
 	b	99f
-15:	ldr	d0, [x3]		/* D1 */
+15:	ldr	d0, [PTR_REG(3)]		/* D1 */
 	b	99f
-16:	ldr	q3, [x3, #48]		/* Q4 */
+16:	ldr	q3, [PTR_REG(3), #48]		/* Q4 */
 	nop
-17:	ldr	q2, [x3, #32]		/* Q3 */
+17:	ldr	q2, [PTR_REG(3), #32]		/* Q3 */
 	nop
-18:	ldp	q0, q1, [x3]		/* Q2 */
+18:	ldp	q0, q1, [PTR_REG(3)]		/* Q2 */
 	b	99f
-19:	ldr	q0, [x3]		/* Q1 */
+19:	ldr	q0, [PTR_REG(3)]		/* Q1 */
 	b	99f
-20:	ldrb	w0, [x3, #BE(7)]	/* UINT8 */
+20:	ldrb	w0, [PTR_REG(3), #BE(7)]	/* UINT8 */
 	b	99f
 21:	brk	#1000			/* reserved */
 	nop
-22:	ldrh	w0, [x3, #BE(6)]	/* UINT16 */
+22:	ldrh	w0, [PTR_REG(3), #BE(6)]	/* UINT16 */
 	b	99f
 23:	brk	#1000			/* reserved */
 	nop
-24:	ldr	w0, [x3, #BE(4)]	/* UINT32 */
+24:	ldr	w0, [PTR_REG(3), #BE(4)]	/* UINT32 */
 	b	99f
 25:	brk	#1000			/* reserved */
 	nop
-26:	ldrsb	x0, [x3, #BE(7)]	/* SINT8 */
+26:	ldrsb	x0, [PTR_REG(3), #BE(7)]	/* SINT8 */
 	b	99f
 27:	brk	#1000			/* reserved */
 	nop
-28:	ldrsh	x0, [x3, #BE(6)]	/* SINT16 */
+28:	ldrsh	x0, [PTR_REG(3), #BE(6)]	/* SINT16 */
 	b	99f
 29:	brk	#1000			/* reserved */
 	nop
-30:	ldrsw	x0, [x3, #BE(4)]	/* SINT32 */
+30:	ldrsw	x0, [PTR_REG(3), #BE(4)]	/* SINT32 */
 	nop
 31:					/* reserved */
-99:	ldp     x29, x30, [sp], #ffi_closure_SYSV_FS
+99:	ldp     GP_REG(29), GP_REG(30), [sp], #ffi_closure_SYSV_FS
 	cfi_adjust_cfa_offset (-ffi_closure_SYSV_FS)
-	cfi_restore (x29)
-	cfi_restore (x30)
+	cfi_restore (GP_REG(29))
+	cfi_restore (GP_REG(30))
 	AUTH_LR_AND_RET
 	cfi_endproc
 
-	.globl	CNAME(ffi_closure_SYSV)
-	FFI_HIDDEN(CNAME(ffi_closure_SYSV))
 #ifdef __ELF__
-	.type	CNAME(ffi_closure_SYSV), #function
 	.size	CNAME(ffi_closure_SYSV), . - CNAME(ffi_closure_SYSV)
 #endif
 
 #if defined(FFI_EXEC_STATIC_TRAMP)
 	.align 4
+	.globl	CNAME(ffi_closure_SYSV_V_alt)
+	FFI_HIDDEN(CNAME(ffi_closure_SYSV_V_alt))
+#ifdef __ELF__
+	.type	CNAME(ffi_closure_SYSV_V_alt), #function
+#endif
 CNAME(ffi_closure_SYSV_V_alt):
 	/* See the comments above trampoline_code_table. */
-	ldr	x17, [sp, #8]			/* Load closure in x17 */
+	ldr	GP_REG(17), [sp, #GP_REG_SIZE]			/* Load closure in x17 */
 	add	sp, sp, #16			/* Restore the stack */
 	b	CNAME(ffi_closure_SYSV_V)
 
-	.globl	CNAME(ffi_closure_SYSV_V_alt)
-	FFI_HIDDEN(CNAME(ffi_closure_SYSV_V_alt))
 #ifdef __ELF__
-	.type	CNAME(ffi_closure_SYSV_V_alt), #function
 	.size	CNAME(ffi_closure_SYSV_V_alt), . - CNAME(ffi_closure_SYSV_V_alt)
 #endif
 
 	.align 4
+	.globl	CNAME(ffi_closure_SYSV_alt)
+	FFI_HIDDEN(CNAME(ffi_closure_SYSV_alt))
+#ifdef __ELF__
+	.type	CNAME(ffi_closure_SYSV_alt), #function
+#endif
 CNAME(ffi_closure_SYSV_alt):
 	/* See the comments above trampoline_code_table. */
-	ldr	x17, [sp, #8]			/* Load closure in x17 */
+	ldr	GP_REG(17), [sp, #GP_REG_SIZE]			/* Load closure in x17 */
 	add	sp, sp, #16			/* Restore the stack */
 	b	CNAME(ffi_closure_SYSV)
 
-	.globl	CNAME(ffi_closure_SYSV_alt)
-	FFI_HIDDEN(CNAME(ffi_closure_SYSV_alt))
 #ifdef __ELF__
-	.type	CNAME(ffi_closure_SYSV_alt), #function
 	.size	CNAME(ffi_closure_SYSV_alt), . - CNAME(ffi_closure_SYSV_alt)
 #endif
 
@@ -431,22 +479,24 @@ CNAME(ffi_closure_SYSV_alt):
  * - restore the stack pointer to what it was when the trampoline was invoked.
  */
 	.align	AARCH64_TRAMP_MAP_SHIFT
+	.globl CNAME(trampoline_code_table)
+	FFI_HIDDEN(CNAME(trampoline_code_table))
+#ifdef __ELF__
+	.type	CNAME(trampoline_code_table), #function
+#endif
 CNAME(trampoline_code_table):
 	.rept	AARCH64_TRAMP_MAP_SIZE / AARCH64_TRAMP_SIZE
 	sub	sp, sp, #16		/* Make space on the stack */
-	str	x17, [sp]		/* Save x17 on stack */
-	adr	x17, #16376		/* Get data address */
-	ldr	x17, [x17]		/* Copy data into x17 */
-	str	x17, [sp, #8]		/* Save data on stack */
-	adr	x17, #16372		/* Get code address */
-	ldr	x17, [x17]		/* Load code address into x17 */
-	br	x17			/* Jump to code */
+	str	GP_REG(17), [sp]		/* Save x17 on stack */
+	adr	GP_REG(17), #16376		/* Get data address */
+	ldr	GP_REG(17), [PTR_REG(17)]		/* Copy data into x17 */
+	str	GP_REG(17), [sp, #GP_REG_SIZE]		/* Save data on stack */
+	adr	GP_REG(17), #16372		/* Get code address */
+	ldr	GP_REG(17), [PTR_REG(17)]		/* Load code address into x17 */
+	br	BR_PTR_REG(17)			/* Jump to code */
 	.endr
 
-	.globl CNAME(trampoline_code_table)
-	FFI_HIDDEN(CNAME(trampoline_code_table))
 #ifdef __ELF__
-	.type	CNAME(trampoline_code_table), #function
 	.size	CNAME(trampoline_code_table), . - CNAME(trampoline_code_table)
 #endif
 	.align	AARCH64_TRAMP_MAP_SHIFT
@@ -457,18 +507,20 @@ CNAME(trampoline_code_table):
 #ifdef __MACH__
 #include <mach/machine/vm_param.h>
     .align PAGE_MAX_SHIFT
+.globl CNAME(ffi_closure_trampoline_table_page)
+FFI_HIDDEN(CNAME(ffi_closure_trampoline_table_page))
+#ifdef __ELF__
+	.type	CNAME(ffi_closure_trampoline_table_page), #function
+#endif
 CNAME(ffi_closure_trampoline_table_page):
     .rept PAGE_MAX_SIZE / FFI_TRAMPOLINE_SIZE
-    adr x16, -PAGE_MAX_SIZE
-    ldp x17, x16, [x16]
-    br x16
+    adr PTR_REG(16), -PAGE_MAX_SIZE
+    ldp GP_REG(17), GP_REG(16), [PTR_REG(16)]
+    br BR_PTR_REG(16)
 	nop		/* each entry in the trampoline config page is 2*sizeof(void*) so the trampoline itself cannot be smaller than 16 bytes */
     .endr
 
-    .globl CNAME(ffi_closure_trampoline_table_page)
-    FFI_HIDDEN(CNAME(ffi_closure_trampoline_table_page))
     #ifdef __ELF__
-    	.type	CNAME(ffi_closure_trampoline_table_page), #function
     	.size	CNAME(ffi_closure_trampoline_table_page), . - CNAME(ffi_closure_trampoline_table_page)
     #endif
 #endif
@@ -477,12 +529,17 @@ CNAME(ffi_closure_trampoline_table_page):
 
 #ifdef FFI_GO_CLOSURES
 	.align 4
+	.globl	CNAME(ffi_go_closure_SYSV_V)
+	FFI_HIDDEN(CNAME(ffi_go_closure_SYSV_V))
+#ifdef __ELF__
+	.type	CNAME(ffi_go_closure_SYSV_V), #function
+#endif
 CNAME(ffi_go_closure_SYSV_V):
 	cfi_startproc
-	stp     x29, x30, [sp, #-ffi_closure_SYSV_FS]!
+	stp     GP_REG(29), GP_REG(30), [sp, #-ffi_closure_SYSV_FS]!
 	cfi_adjust_cfa_offset (ffi_closure_SYSV_FS)
-	cfi_rel_offset (x29, 0)
-	cfi_rel_offset (x30, 8)
+	cfi_rel_offset (GP_REG(29), 0)
+	cfi_rel_offset (GP_REG(30), GP_REG_SIZE)
 
 	/* Save the argument passing vector registers.  */
 	stp     q0, q1, [sp, #16 + 0]
@@ -492,39 +549,38 @@ CNAME(ffi_go_closure_SYSV_V):
 	b	0f
 	cfi_endproc
 
-	.globl	CNAME(ffi_go_closure_SYSV_V)
-	FFI_HIDDEN(CNAME(ffi_go_closure_SYSV_V))
 #ifdef __ELF__
-	.type	CNAME(ffi_go_closure_SYSV_V), #function
 	.size	CNAME(ffi_go_closure_SYSV_V), . - CNAME(ffi_go_closure_SYSV_V)
 #endif
 
 	.align	4
+	.globl	CNAME(ffi_go_closure_SYSV)
+	FFI_HIDDEN(CNAME(ffi_go_closure_SYSV))
+#ifdef __ELF__
+	.type	CNAME(ffi_go_closure_SYSV), #function
+#endif
 	cfi_startproc
 CNAME(ffi_go_closure_SYSV):
-	stp     x29, x30, [sp, #-ffi_closure_SYSV_FS]!
+	stp     GP_REG(29), GP_REG(30), [sp, #-ffi_closure_SYSV_FS]!
 	cfi_adjust_cfa_offset (ffi_closure_SYSV_FS)
-	cfi_rel_offset (x29, 0)
-	cfi_rel_offset (x30, 8)
+	cfi_rel_offset (GP_REG(29), 0)
+	cfi_rel_offset (GP_REG(30), GP_REG_SIZE)
 0:
-	mov     x29, sp
+	mov     GP_REG(29), sp
 
 	/* Save the argument passing core registers.  */
-	stp     x0, x1, [sp, #16 + 16*N_V_ARG_REG + 0]
-	stp     x2, x3, [sp, #16 + 16*N_V_ARG_REG + 16]
-	stp     x4, x5, [sp, #16 + 16*N_V_ARG_REG + 32]
-	stp     x6, x7, [sp, #16 + 16*N_V_ARG_REG + 48]
+	stp     GP_REG(0), GP_REG(1), [sp, #16 + 16*N_V_ARG_REG + 0*GP_REG_SIZE]
+	stp     GP_REG(2), GP_REG(3), [sp, #16 + 16*N_V_ARG_REG + 2*GP_REG_SIZE]
+	stp     GP_REG(4), GP_REG(5), [sp, #16 + 16*N_V_ARG_REG + 4*GP_REG_SIZE]
+	stp     GP_REG(6), GP_REG(7), [sp, #16 + 16*N_V_ARG_REG + 6*GP_REG_SIZE]
 
 	/* Load ffi_closure_inner arguments.  */
-	ldp	PTR_REG(0), PTR_REG(1), [x18, #PTR_SIZE]/* load cif, fn */
-	mov	x2, x18					/* load user_data */
+	ldp	PTR_REG(0), PTR_REG(1), [PTR_REG(18), #PTR_SIZE]/* load cif, fn */
+	mov	GP_REG(2), GP_REG(18)					/* load user_data */
 	b	.Ldo_closure
 	cfi_endproc
 
-	.globl	CNAME(ffi_go_closure_SYSV)
-	FFI_HIDDEN(CNAME(ffi_go_closure_SYSV))
 #ifdef __ELF__
-	.type	CNAME(ffi_go_closure_SYSV), #function
 	.size	CNAME(ffi_go_closure_SYSV), . - CNAME(ffi_go_closure_SYSV)
 #endif
 #endif /* FFI_GO_CLOSURES */
diff --git src/dlmalloc.c src/dlmalloc.c
index 6f4a5f6..28f3761 100644
--- src/dlmalloc.c
+++ src/dlmalloc.c
@@ -1278,7 +1278,9 @@ extern void*     sbrk(ptrdiff_t);
 #define CHUNK_ALIGN_MASK    (MALLOC_ALIGNMENT - SIZE_T_ONE)
 
 /* True if address a has acceptable alignment */
+#ifndef is_aligned
 #define is_aligned(A)       (((size_t)((A)) & (CHUNK_ALIGN_MASK)) == 0)
+#endif
 
 /* the number of bytes to offset an address to align it */
 #define align_offset(A)\
@@ -3947,10 +3949,7 @@ static void* internal_memalign(mstate m, size_t alignment, size_t bytes) {
           We've allocated enough total room so that this is always
           possible.
         */
-        char* br = (char*)mem2chunk((size_t)(((size_t)(mem +
-                                                       alignment -
-                                                       SIZE_T_ONE)) &
-                                             -alignment));
+        char* br = (char*)mem2chunk(FFI_ALIGN(mem, alignment));
         char* pos = ((size_t)(br - (char*)(p)) >= MIN_CHUNK_SIZE)?
           br : br+alignment;
         mchunkptr newp = (mchunkptr)pos;
diff --git src/riscv/ffi.c src/riscv/ffi.c
index b455b12..f8c248e 100644
--- src/riscv/ffi.c
+++ src/riscv/ffi.c
@@ -50,9 +50,15 @@ typedef struct call_context
 #if ABI_FLEN
     ABI_FLOAT fa[8];
 #endif
-    size_t a[8];
+    uintptr_t a[8];
     /* used by the assembly code to in-place construct its own stack frame */
-    char frame[16];
+    struct {
+#if __SIZEOF_POINTER__ < 8
+        void* pad[2];
+#endif
+        void *saved_fp;
+        void *saved_ra;
+    } frame;
 } call_context;
 
 typedef struct call_builder
@@ -60,13 +66,13 @@ typedef struct call_builder
     call_context *aregs;
     int used_integer;
     int used_float;
-    size_t *used_stack;
+    uintptr_t *used_stack;
     void *struct_stack;
 } call_builder;
 
 /* integer (not pointer) less than ABI XLEN */
 /* FFI_TYPE_INT does not appear to be used */
-#if __SIZEOF_POINTER__ == 8
+#if __riscv_xlen >= 64
 #define IS_INT(type) ((type) >= FFI_TYPE_UINT8 && (type) <= FFI_TYPE_SINT64)
 #else
 #define IS_INT(type) ((type) >= FFI_TYPE_UINT8 && (type) <= FFI_TYPE_SINT32)
@@ -131,7 +137,7 @@ static float_struct_info struct_passed_as_elements(call_builder *cb, ffi_type *t
 
 /* allocates a single register, float register, or XLEN-sized stack slot to a datum */
 static void marshal_atom(call_builder *cb, int type, void *data) {
-    size_t value = 0;
+    uintptr_t value = 0;
     switch (type) {
         case FFI_TYPE_UINT8: value = *(uint8_t *)data; break;
         case FFI_TYPE_SINT8: value = *(int8_t *)data; break;
@@ -140,11 +146,11 @@ static void marshal_atom(call_builder *cb, int type, void *data) {
         /* 32-bit quantities are always sign-extended in the ABI */
         case FFI_TYPE_UINT32: value = *(int32_t *)data; break;
         case FFI_TYPE_SINT32: value = *(int32_t *)data; break;
-#if __SIZEOF_POINTER__ == 8
+#if __riscv_xlen >= 64
         case FFI_TYPE_UINT64: value = *(uint64_t *)data; break;
         case FFI_TYPE_SINT64: value = *(int64_t *)data; break;
 #endif
-        case FFI_TYPE_POINTER: value = *(size_t *)data; break;
+        case FFI_TYPE_POINTER: value = *(uintptr_t *)data; break;
 
         /* float values may be recoded in an implementation-defined way
            by hardware conforming to 2.1 or earlier, so use asm to
@@ -170,7 +176,7 @@ static void marshal_atom(call_builder *cb, int type, void *data) {
 }
 
 static void unmarshal_atom(call_builder *cb, int type, void *data) {
-    size_t value;
+    uintptr_t value;
     switch (type) {
 #if ABI_FLEN >= 32
         case FFI_TYPE_FLOAT:
@@ -191,17 +197,17 @@ static void unmarshal_atom(call_builder *cb, int type, void *data) {
     }
 
     switch (type) {
-        case FFI_TYPE_UINT8: *(uint8_t *)data = value; break;
-        case FFI_TYPE_SINT8: *(uint8_t *)data = value; break;
-        case FFI_TYPE_UINT16: *(uint16_t *)data = value; break;
-        case FFI_TYPE_SINT16: *(uint16_t *)data = value; break;
-        case FFI_TYPE_UINT32: *(uint32_t *)data = value; break;
-        case FFI_TYPE_SINT32: *(uint32_t *)data = value; break;
-#if __SIZEOF_POINTER__ == 8
-        case FFI_TYPE_UINT64: *(uint64_t *)data = value; break;
-        case FFI_TYPE_SINT64: *(uint64_t *)data = value; break;
+        case FFI_TYPE_UINT8: *(uint8_t *)data = (uint8_t)value; break;
+        case FFI_TYPE_SINT8: *(uint8_t *)data = (uint8_t)value; break;
+        case FFI_TYPE_UINT16: *(uint16_t *)data = (uint16_t)value; break;
+        case FFI_TYPE_SINT16: *(uint16_t *)data = (uint16_t)value; break;
+        case FFI_TYPE_UINT32: *(uint32_t *)data = (uint32_t)value; break;
+        case FFI_TYPE_SINT32: *(uint32_t *)data = (uint32_t)value; break;
+#if __riscv_xlen >= 64
+        case FFI_TYPE_UINT64: *(uint64_t *)data = (uint64_t)value; break;
+        case FFI_TYPE_SINT64: *(uint64_t *)data = (uint64_t)value; break;
 #endif
-        case FFI_TYPE_POINTER: *(size_t *)data = value; break;
+        case FFI_TYPE_POINTER: *(uintptr_t *)data = value; break;
         default: FFI_ASSERT(0); break;
     }
 }
@@ -242,7 +248,7 @@ static void marshal(call_builder *cb, ffi_type *type, int var, void *data) {
         if (type->alignment > __SIZEOF_POINTER__) {
             if (var)
                 cb->used_integer = FFI_ALIGN(cb->used_integer, 2);
-            cb->used_stack = (size_t *)FFI_ALIGN(cb->used_stack, 2*__SIZEOF_POINTER__);
+            cb->used_stack = (uintptr_t *)FFI_ALIGN(cb->used_stack, 2*__SIZEOF_POINTER__);
         }
 
         memcpy(realign, data, type->size);
@@ -290,7 +296,7 @@ static void *unmarshal(call_builder *cb, ffi_type *type, int var, void *data) {
         if (type->alignment > __SIZEOF_POINTER__) {
             if (var)
                 cb->used_integer = FFI_ALIGN(cb->used_integer, 2);
-            cb->used_stack = (size_t *)FFI_ALIGN(cb->used_stack, 2*__SIZEOF_POINTER__);
+            cb->used_stack = (uintptr_t *)FFI_ALIGN(cb->used_stack, 2*__SIZEOF_POINTER__);
         }
 
         if (type->size > 0)
@@ -337,7 +343,7 @@ ffi_call_int (ffi_cif *cif, void (*fn) (void), void *rvalue, void **avalue,
     /* this is a conservative estimate, assuming a complex return value and
        that all remaining arguments are long long / __int128 */
     size_t arg_bytes = cif->nargs <= 3 ? 0 :
-        FFI_ALIGN(2 * sizeof(size_t) * (cif->nargs - 3), STKALIGN);
+        FFI_ALIGN(2 * sizeof(uintptr_t) * (cif->nargs - 3), STKALIGN);
     /* Allocate space for copies of big structures.  */
     size_t struct_bytes = FFI_ALIGN (cif->bytes, STKALIGN);
     size_t rval_bytes = 0;
@@ -348,12 +354,12 @@ ffi_call_int (ffi_cif *cif, void (*fn) (void), void *rvalue, void **avalue,
     /* the assembly code will deallocate all stack data at lower addresses
        than the argument region, so we need to allocate the frame and the
        return value after the arguments in a single allocation */
-    size_t alloc_base;
+    char* alloc_base;
     /* Argument region must be 16-byte aligned */
     if (_Alignof(max_align_t) >= STKALIGN) {
         /* since sizeof long double is normally 16, the compiler will
            guarantee alloca alignment to at least that much */
-        alloc_base = (size_t)alloca(alloc_size);
+        alloc_base = alloca(alloc_size);
     } else {
         alloc_base = FFI_ALIGN(alloca(alloc_size + STKALIGN - 1), STKALIGN);
     }
@@ -364,6 +370,11 @@ ffi_call_int (ffi_cif *cif, void (*fn) (void), void *rvalue, void **avalue,
     call_builder cb;
     cb.used_float = cb.used_integer = 0;
     cb.aregs = (call_context*)(alloc_base + arg_bytes + rval_bytes + struct_bytes);
+#ifdef __CHERI_PURE_CAPABILITY__
+    cb.aregs = __builtin_cheri_bounds_set(cb.aregs, sizeof(*cb.aregs));
+#endif
+    /* Initialize the frame with 0xaa to detect errors. */
+    memset(cb.aregs, 0xaa, sizeof(*cb.aregs));
     cb.used_stack = (void*)alloc_base;
     cb.struct_stack = (void *) (alloc_base + arg_bytes + rval_bytes);
 
@@ -375,6 +386,11 @@ ffi_call_int (ffi_cif *cif, void (*fn) (void), void *rvalue, void **avalue,
     for (i = 0; i < cif->nargs; i++)
         marshal(&cb, cif->arg_types[i], i >= cif->riscv_nfixedargs, avalue[i]);
 
+    /* Start using the stack from the beginning of the alloca() used for
+     * arguments. We have to restore the original stack bounds so that the
+     * called function has a valid stack instead of an out-of-bounds pointer. */
+    alloc_base = __builtin_cheri_address_set(__builtin_cheri_stack_get(),
+                                             (ptraddr_t)alloc_base);
     ffi_call_asm ((void *) alloc_base, cb.aregs, fn, closure);
 
     cb.used_float = cb.used_integer = 0;
@@ -434,10 +450,18 @@ ffi_status ffi_prep_closure_loc(ffi_closure *closure, ffi_cif *cif, void (*fun)(
        as the memory is readable it should work */
 
     tramp[0] = 0x00000317; /* auipc t1, 0 (i.e. t0 <- codeloc) */
+#ifdef __CHERI_PURE_CAPABILITY__
+#if __riscv_xlen == 64
+    tramp[1] = 0x0103238f; /* RV64: clc ct2, 16(ct1) */
+#else
+    tramp[1] = 0x01033383; /* RV32: clc ct2, 16(ct1) */
+#endif
+#else
 #if __SIZEOF_POINTER__ == 8
     tramp[1] = 0x01033383; /* ld t2, 16(t1) */
 #else
     tramp[1] = 0x01032383; /* lw t2, 16(t1) */
+#endif
 #endif
     tramp[2] = 0x00038067; /* jr t2 */
     tramp[3] = 0x00000013; /* nop */
@@ -478,7 +502,7 @@ void FFI_HIDDEN
 ffi_closure_inner (ffi_cif *cif,
 		   void (*fun) (ffi_cif *, void *, void **, void *),
 		   void *user_data,
-		   size_t *stack, call_context *aregs)
+		   uintptr_t *stack, call_context *aregs)
 {
     void **avalue = alloca(cif->nargs * sizeof(void*));
     /* storage for arguments which will be copied by unmarshal().  We could
diff --git src/riscv/sysv.S src/riscv/sysv.S
index 522d0b0..3f7fee4 100644
--- src/riscv/sysv.S
+++ src/riscv/sysv.S
@@ -32,7 +32,22 @@
 
 /* Define aliases so that we can handle all ABIs uniformly */
 
-#if __SIZEOF_POINTER__ == 8
+#ifdef __CHERI_PURE_CAPABILITY__
+#define PTRREG(reg) c##reg
+/* Could also use .macro to make it look more like an assembly instruction */
+#define PTRADDI(dst, src, inc) cincoffset PTRREG(dst), PTRREG(src), inc
+#define PTRMOVE(dst, src) cmove PTRREG(dst), PTRREG(src)
+#else
+#define PTRREG(reg) reg
+#define PTRADDI(dst, src, inc) addi dst, src, inc
+#define PTRMOVE(dst, src) mv dst, src
+#endif
+
+#ifdef __CHERI_PURE_CAPABILITY__
+#define PTRS __SIZEOF_POINTER__
+#define LARG clc
+#define SARG csc
+#elif __SIZEOF_POINTER__ == 8
 #define PTRS 8
 #define LARG ld
 #define SARG sd
@@ -42,10 +57,18 @@
 #define SARG sw
 #endif
 
+#define LOAD_ARG(dst, imm, base) LARG PTRREG(dst), imm(PTRREG(base))
+#define STORE_ARG(value, imm, base) SARG PTRREG(value), imm(PTRREG(base))
+
 #if __riscv_float_abi_double
 #define FLTS 8
+#ifdef __CHERI_PURE_CAPABILITY__
+#define FLARG cfld
+#define FSARG cfsd
+#else
 #define FLARG fld
 #define FSARG fsd
+#endif
 #elif __riscv_float_abi_single
 #define FLTS 4
 #define FLARG flw
@@ -54,6 +77,11 @@
 #define FLTS 0
 #endif
 
+#if FLTS
+#define LOAD_FLOAT(dst, imm, base) FLARG dst, imm(PTRREG(base))
+#define STORE_FLOAT(value, imm, base) FSARG value, imm(PTRREG(base))
+#endif
+
 #define fp s0
 
     .text
@@ -71,7 +99,13 @@
                      void (*fn) (void), void *closure);
 */
 
-#define FRAME_LEN (8 * FLTS + 8 * PTRS + 16)
+#if __SIZEOF_POINTER__ < 8
+#define FRAME_PADDING 8
+#else
+#define FRAME_PADDING 0
+#endif
+
+#define FRAME_LEN (8 * FLTS + 8 * PTRS + FRAME_PADDING + 2 * PTRS)
 
 ffi_call_asm:
     .cfi_startproc
@@ -87,59 +121,71 @@ ffi_call_asm:
     */
 
     .cfi_def_cfa 11, FRAME_LEN # interim CFA based on a1
-    SARG    fp, FRAME_LEN - 2*PTRS(a1)
+    STORE_ARG(fp, FRAME_LEN - 2*PTRS, a1)
     .cfi_offset 8, -2*PTRS
-    SARG    ra, FRAME_LEN - 1*PTRS(a1)
+    STORE_ARG(ra, FRAME_LEN - 1*PTRS, a1)
     .cfi_offset 1, -1*PTRS
 
-    addi    fp, a1, FRAME_LEN
-    mv      sp, a0
+    PTRADDI(fp, a1, FRAME_LEN)
+#ifdef __CHERI_PURE_CAPABILITY__
+    /* Restore csp bounds on cfp */
+    csetaddr cfp, csp, fp
+#endif
+    PTRMOVE(sp, a0)
     .cfi_def_cfa 8, 0 # our frame is fully set up
 
     # Load arguments
-    mv      t1, a2
-    mv      t2, a3
+    PTRMOVE(t1, a2)
+    PTRMOVE(t2, a3)
 
 #if FLTS
-    FLARG   fa0, -FRAME_LEN+0*FLTS(fp)
-    FLARG   fa1, -FRAME_LEN+1*FLTS(fp)
-    FLARG   fa2, -FRAME_LEN+2*FLTS(fp)
-    FLARG   fa3, -FRAME_LEN+3*FLTS(fp)
-    FLARG   fa4, -FRAME_LEN+4*FLTS(fp)
-    FLARG   fa5, -FRAME_LEN+5*FLTS(fp)
-    FLARG   fa6, -FRAME_LEN+6*FLTS(fp)
-    FLARG   fa7, -FRAME_LEN+7*FLTS(fp)
+    LOAD_FLOAT(fa0, -FRAME_LEN+0*FLTS, fp)
+    LOAD_FLOAT(fa1, -FRAME_LEN+1*FLTS, fp)
+    LOAD_FLOAT(fa2, -FRAME_LEN+2*FLTS, fp)
+    LOAD_FLOAT(fa3, -FRAME_LEN+3*FLTS, fp)
+    LOAD_FLOAT(fa4, -FRAME_LEN+4*FLTS, fp)
+    LOAD_FLOAT(fa5, -FRAME_LEN+5*FLTS, fp)
+    LOAD_FLOAT(fa6, -FRAME_LEN+6*FLTS, fp)
+    LOAD_FLOAT(fa7, -FRAME_LEN+7*FLTS, fp)
 #endif
 
-    LARG    a0, -FRAME_LEN+8*FLTS+0*PTRS(fp)
-    LARG    a1, -FRAME_LEN+8*FLTS+1*PTRS(fp)
-    LARG    a2, -FRAME_LEN+8*FLTS+2*PTRS(fp)
-    LARG    a3, -FRAME_LEN+8*FLTS+3*PTRS(fp)
-    LARG    a4, -FRAME_LEN+8*FLTS+4*PTRS(fp)
-    LARG    a5, -FRAME_LEN+8*FLTS+5*PTRS(fp)
-    LARG    a6, -FRAME_LEN+8*FLTS+6*PTRS(fp)
-    LARG    a7, -FRAME_LEN+8*FLTS+7*PTRS(fp)
+    LOAD_ARG(a0, -FRAME_LEN+8*FLTS+0*PTRS, fp)
+    LOAD_ARG(a1, -FRAME_LEN+8*FLTS+1*PTRS, fp)
+    LOAD_ARG(a2, -FRAME_LEN+8*FLTS+2*PTRS, fp)
+    LOAD_ARG(a3, -FRAME_LEN+8*FLTS+3*PTRS, fp)
+    LOAD_ARG(a4, -FRAME_LEN+8*FLTS+4*PTRS, fp)
+    LOAD_ARG(a5, -FRAME_LEN+8*FLTS+5*PTRS, fp)
+    LOAD_ARG(a6, -FRAME_LEN+8*FLTS+6*PTRS, fp)
+    LOAD_ARG(a7, -FRAME_LEN+8*FLTS+7*PTRS, fp)
 
     /* Call */
+#ifdef __CHERI_PURE_CAPABILITY__
+    cjalr   ct1
+#else
     jalr    t1
+#endif
 
     /* Save return values - only a0/a1 (fa0/fa1) are used */
 #if FLTS
-    FSARG   fa0, -FRAME_LEN+0*FLTS(fp)
-    FSARG   fa1, -FRAME_LEN+1*FLTS(fp)
+    STORE_FLOAT(fa0, -FRAME_LEN+0*FLTS, fp)
+    STORE_FLOAT(fa1, -FRAME_LEN+1*FLTS, fp)
 #endif
 
-    SARG    a0, -FRAME_LEN+8*FLTS+0*PTRS(fp)
-    SARG    a1, -FRAME_LEN+8*FLTS+1*PTRS(fp)
+    STORE_ARG(a0, -FRAME_LEN+8*FLTS+0*PTRS, fp)
+    STORE_ARG(a1, -FRAME_LEN+8*FLTS+1*PTRS, fp)
 
     /* Restore and return */
-    addi    sp, fp, -FRAME_LEN
+    PTRADDI(sp, fp, -FRAME_LEN)
     .cfi_def_cfa 2, FRAME_LEN
-    LARG    ra, -1*PTRS(fp)
+    LOAD_ARG(ra, -1*PTRS, fp)
     .cfi_restore 1
-    LARG    fp, -2*PTRS(fp)
+    LOAD_ARG(fp, -2*PTRS, fp)
     .cfi_restore 8
+#ifdef __CHERI_PURE_CAPABILITY__
+    cret
+#else
     ret
+#endif
     .cfi_endproc
     .size   ffi_call_asm, .-ffi_call_asm
 
@@ -158,63 +204,72 @@ ffi_call_asm:
 ffi_closure_asm:
     .cfi_startproc
 
-    addi    sp,  sp, -FRAME_LEN
+    PTRADDI(sp,  sp, -FRAME_LEN)
     .cfi_def_cfa_offset FRAME_LEN
 
     /* make a frame */
-    SARG    fp, FRAME_LEN - 2*PTRS(sp)
+    STORE_ARG(fp, FRAME_LEN - 2*PTRS, sp)
     .cfi_offset 8, -2*PTRS
-    SARG    ra, FRAME_LEN - 1*PTRS(sp)
+    STORE_ARG(ra, FRAME_LEN - 1*PTRS, sp)
     .cfi_offset 1, -1*PTRS
-    addi    fp, sp, FRAME_LEN
+    PTRADDI(fp, sp, FRAME_LEN)
 
     /* save arguments */
 #if FLTS
-    FSARG   fa0, 0*FLTS(sp)
-    FSARG   fa1, 1*FLTS(sp)
-    FSARG   fa2, 2*FLTS(sp)
-    FSARG   fa3, 3*FLTS(sp)
-    FSARG   fa4, 4*FLTS(sp)
-    FSARG   fa5, 5*FLTS(sp)
-    FSARG   fa6, 6*FLTS(sp)
-    FSARG   fa7, 7*FLTS(sp)
+    STORE_FLOAT(fa0, 0*FLTS, sp)
+    STORE_FLOAT(fa1, 1*FLTS, sp)
+    STORE_FLOAT(fa2, 2*FLTS, sp)
+    STORE_FLOAT(fa3, 3*FLTS, sp)
+    STORE_FLOAT(fa4, 4*FLTS, sp)
+    STORE_FLOAT(fa5, 5*FLTS, sp)
+    STORE_FLOAT(fa6, 6*FLTS, sp)
+    STORE_FLOAT(fa7, 7*FLTS, sp)
 #endif
 
-    SARG    a0, 8*FLTS+0*PTRS(sp)
-    SARG    a1, 8*FLTS+1*PTRS(sp)
-    SARG    a2, 8*FLTS+2*PTRS(sp)
-    SARG    a3, 8*FLTS+3*PTRS(sp)
-    SARG    a4, 8*FLTS+4*PTRS(sp)
-    SARG    a5, 8*FLTS+5*PTRS(sp)
-    SARG    a6, 8*FLTS+6*PTRS(sp)
-    SARG    a7, 8*FLTS+7*PTRS(sp)
+    STORE_ARG(a0, 8*FLTS+0*PTRS, sp)
+    STORE_ARG(a1, 8*FLTS+1*PTRS, sp)
+    STORE_ARG(a2, 8*FLTS+2*PTRS, sp)
+    STORE_ARG(a3, 8*FLTS+3*PTRS, sp)
+    STORE_ARG(a4, 8*FLTS+4*PTRS, sp)
+    STORE_ARG(a5, 8*FLTS+5*PTRS, sp)
+    STORE_ARG(a6, 8*FLTS+6*PTRS, sp)
+    STORE_ARG(a7, 8*FLTS+7*PTRS, sp)
 
     /* enter C */
-    LARG    a0, FFI_TRAMPOLINE_SIZE+0*PTRS(t1)
-    LARG    a1, FFI_TRAMPOLINE_SIZE+1*PTRS(t1)
-    LARG    a2, FFI_TRAMPOLINE_SIZE+2*PTRS(t1)
-    addi    a3, sp, FRAME_LEN
-    mv      a4, sp
-
+    LOAD_ARG(a0, FFI_TRAMPOLINE_SIZE+0*PTRS, t1)
+    LOAD_ARG(a1, FFI_TRAMPOLINE_SIZE+1*PTRS, t1)
+    LOAD_ARG(a2, FFI_TRAMPOLINE_SIZE+2*PTRS, t1)
+    PTRADDI(a3, sp, FRAME_LEN)
+    PTRMOVE(a4, sp)
+
+#ifdef __CHERI_PURE_CAPABILITY__
+    cllc    cra, ffi_closure_inner
+    cjalr   cra
+#else
     call    ffi_closure_inner
+#endif
 
     /* return values */
 #if FLTS
-    FLARG   fa0, 0*FLTS(sp)
-    FLARG   fa1, 1*FLTS(sp)
+    LOAD_FLOAT(fa0, 0*FLTS, sp)
+    LOAD_FLOAT(fa1, 1*FLTS, sp)
 #endif
 
-    LARG    a0, 8*FLTS+0*PTRS(sp)
-    LARG    a1, 8*FLTS+1*PTRS(sp)
+    LOAD_ARG(a0, 8*FLTS+0*PTRS, sp)
+    LOAD_ARG(a1, 8*FLTS+1*PTRS, sp)
 
     /* restore and return */
-    LARG    ra, FRAME_LEN-1*PTRS(sp)
+    LOAD_ARG(ra, FRAME_LEN-1*PTRS, sp)
     .cfi_restore 1
-    LARG    fp, FRAME_LEN-2*PTRS(sp)
+    LOAD_ARG(fp, FRAME_LEN-2*PTRS, sp)
     .cfi_restore 8
-    addi    sp, sp, FRAME_LEN
+    PTRADDI(sp, sp, FRAME_LEN)
     .cfi_def_cfa_offset 0
+#ifdef __CHERI_PURE_CAPABILITY__
+    cret
+#else
     ret
+#endif
     .cfi_endproc
     .size ffi_closure_asm, .-ffi_closure_asm
 
@@ -232,62 +287,71 @@ ffi_closure_asm:
 ffi_go_closure_asm:
     .cfi_startproc
 
-    addi    sp,  sp, -FRAME_LEN
+    PTRADDI(sp,  sp, -FRAME_LEN)
     .cfi_def_cfa_offset FRAME_LEN
 
     /* make a frame */
-    SARG    fp, FRAME_LEN - 2*PTRS(sp)
+    STORE_ARG(fp, FRAME_LEN - 2*PTRS, sp)
     .cfi_offset 8, -2*PTRS
-    SARG    ra, FRAME_LEN - 1*PTRS(sp)
+    STORE_ARG(ra, FRAME_LEN - 1*PTRS, sp)
     .cfi_offset 1, -1*PTRS
-    addi    fp, sp, FRAME_LEN
+    PTRADDI(fp, sp, FRAME_LEN)
 
     /* save arguments */
 #if FLTS
-    FSARG   fa0, 0*FLTS(sp)
-    FSARG   fa1, 1*FLTS(sp)
-    FSARG   fa2, 2*FLTS(sp)
-    FSARG   fa3, 3*FLTS(sp)
-    FSARG   fa4, 4*FLTS(sp)
-    FSARG   fa5, 5*FLTS(sp)
-    FSARG   fa6, 6*FLTS(sp)
-    FSARG   fa7, 7*FLTS(sp)
+    STORE_FLOAT(fa0, 0*FLTS, sp)
+    STORE_FLOAT(fa1, 1*FLTS, sp)
+    STORE_FLOAT(fa2, 2*FLTS, sp)
+    STORE_FLOAT(fa3, 3*FLTS, sp)
+    STORE_FLOAT(fa4, 4*FLTS, sp)
+    STORE_FLOAT(fa5, 5*FLTS, sp)
+    STORE_FLOAT(fa6, 6*FLTS, sp)
+    STORE_FLOAT(fa7, 7*FLTS, sp)
 #endif
 
-    SARG    a0, 8*FLTS+0*PTRS(sp)
-    SARG    a1, 8*FLTS+1*PTRS(sp)
-    SARG    a2, 8*FLTS+2*PTRS(sp)
-    SARG    a3, 8*FLTS+3*PTRS(sp)
-    SARG    a4, 8*FLTS+4*PTRS(sp)
-    SARG    a5, 8*FLTS+5*PTRS(sp)
-    SARG    a6, 8*FLTS+6*PTRS(sp)
-    SARG    a7, 8*FLTS+7*PTRS(sp)
+    STORE_ARG(a0, 8*FLTS+0*PTRS, sp)
+    STORE_ARG(a1, 8*FLTS+1*PTRS, sp)
+    STORE_ARG(a2, 8*FLTS+2*PTRS, sp)
+    STORE_ARG(a3, 8*FLTS+3*PTRS, sp)
+    STORE_ARG(a4, 8*FLTS+4*PTRS, sp)
+    STORE_ARG(a5, 8*FLTS+5*PTRS, sp)
+    STORE_ARG(a6, 8*FLTS+6*PTRS, sp)
+    STORE_ARG(a7, 8*FLTS+7*PTRS, sp)
 
     /* enter C */
-    LARG    a0, 1*PTRS(t2)
-    LARG    a1, 2*PTRS(t2)
-    mv      a2, t2
-    addi    a3, sp, FRAME_LEN
-    mv      a4, sp
-
+    LOAD_ARG(a0, 1*PTRS, t2)
+    LOAD_ARG(a1, 2*PTRS, t2)
+    PTRMOVE(a2, t2)
+    PTRADDI(a3, sp, FRAME_LEN)
+    PTRMOVE(a4, sp)
+
+#ifdef __CHERI_PURE_CAPABILITY__
+    cllc    cra, ffi_closure_inner
+    cjalr   cra
+#else
     call    ffi_closure_inner
+#endif
 
     /* return values */
 #if FLTS
-    FLARG   fa0, 0*FLTS(sp)
-    FLARG   fa1, 1*FLTS(sp)
+    LOAD_FLOAT(fa0, 0*FLTS, sp)
+    LOAD_FLOAT(fa1, 1*FLTS, sp)
 #endif
 
-    LARG    a0, 8*FLTS+0*PTRS(sp)
-    LARG    a1, 8*FLTS+1*PTRS(sp)
+    LOAD_ARG(a0, 8*FLTS+0*PTRS, sp)
+    LOAD_ARG(a1, 8*FLTS+1*PTRS, sp)
 
     /* restore and return */
-    LARG    ra, FRAME_LEN-1*PTRS(sp)
+    LOAD_ARG(ra, FRAME_LEN-1*PTRS, sp)
     .cfi_restore 1
-    LARG    fp, FRAME_LEN-2*PTRS(sp)
+    LOAD_ARG(fp, FRAME_LEN-2*PTRS, sp)
     .cfi_restore 8
-    addi    sp, sp, FRAME_LEN
+    PTRADDI(sp, sp, FRAME_LEN)
     .cfi_def_cfa_offset 0
+#ifdef __CHERI_PURE_CAPABILITY__
+    cret
+#else
     ret
+#endif
     .cfi_endproc
     .size ffi_go_closure_asm, .-ffi_go_closure_asm
diff --git testsuite/lib/target-libpath.exp testsuite/lib/target-libpath.exp
index e33a656..abb2f10 100644
--- testsuite/lib/target-libpath.exp
+++ testsuite/lib/target-libpath.exp
@@ -24,6 +24,9 @@ set orig_ld_libraryn32_path_saved 0
 set orig_ld_library64_path_saved 0
 set orig_ld_library_path_32_saved 0
 set orig_ld_library_path_64_saved 0
+set orig_ld_64_library_path_saved 0
+set orig_ld_64c_library_path_saved 0
+set orig_ld_64cb_library_path_saved 0
 set orig_dyld_library_path_saved 0
 set orig_path_saved 0
 
@@ -41,6 +44,9 @@ proc set_ld_library_path_env_vars { } {
   global orig_ld_library64_path_saved
   global orig_ld_library_path_32_saved
   global orig_ld_library_path_64_saved
+  global orig_ld_64_library_path_saved
+  global orig_ld_64c_library_path_saved
+  global orig_ld_64cb_library_path_saved
   global orig_dyld_library_path_saved
   global orig_path_saved
   global orig_ld_library_path
@@ -50,6 +56,9 @@ proc set_ld_library_path_env_vars { } {
   global orig_ld_library64_path
   global orig_ld_library_path_32
   global orig_ld_library_path_64
+  global orig_ld_64_library_path
+  global orig_ld_64c_library_path
+  global orig_ld_64cb_library_path
   global orig_dyld_library_path
   global orig_path
   global GCC_EXEC_PREFIX
@@ -98,6 +107,18 @@ proc set_ld_library_path_env_vars { } {
       set orig_ld_library_path_64 "$env(LD_LIBRARY_PATH_64)"
       set orig_ld_library_path_64_saved 1
     }
+    if [info exists env(LD_64_LIBRARY_PATH)] {
+      set orig_ld_64_library_path "$env(LD_64_LIBRARY_PATH)"
+      set orig_ld_64_library_path_saved 1
+    }
+    if [info exists env(LD_64C_LIBRARY_PATH)] {
+      set orig_ld_64c_library_path "$env(LD_64C_LIBRARY_PATH)"
+      set orig_ld_64c_library_path_saved 1
+    }
+    if [info exists env(LD_64CB_LIBRARY_PATH)] {
+      set orig_ld_64cb_library_path "$env(LD_64CB_LIBRARY_PATH)"
+      set orig_ld_64cb_library_path_saved 1
+    }
     if [info exists env(DYLD_LIBRARY_PATH)] {
       set orig_dyld_library_path "$env(DYLD_LIBRARY_PATH)"
       set orig_dyld_library_path_saved 1
@@ -170,6 +191,27 @@ proc set_ld_library_path_env_vars { } {
   } else {
     setenv LD_LIBRARY_PATH_64 "$ld_library_path"
   }
+  if { $orig_ld_64_library_path_saved } {
+    setenv LD_64_LIBRARY_PATH "$ld_library_path:$orig_ld_64_library_path"
+  } elseif { $orig_ld_library_path_saved } {
+    setenv LD_64_LIBRARY_PATH "$ld_library_path:$orig_ld_library_path"
+  } else {
+    setenv LD_64_LIBRARY_PATH "$ld_library_path"
+  }
+  if { $orig_ld_64c_library_path_saved } {
+    setenv LD_64C_LIBRARY_PATH "$ld_library_path:$orig_ld_64c_library_path"
+  } elseif { $orig_ld_library_path_saved } {
+    setenv LD_64C_LIBRARY_PATH "$ld_library_path:$orig_ld_library_path"
+  } else {
+    setenv LD_64C_LIBRARY_PATH "$ld_library_path"
+  }
+  if { $orig_ld_64cb_library_path_saved } {
+    setenv LD_64CB_LIBRARY_PATH "$ld_library_path:$orig_ld_64cb_library_path"
+  } elseif { $orig_ld_library_path_saved } {
+    setenv LD_64CB_LIBRARY_PATH "$ld_library_path:$orig_ld_library_path"
+  } else {
+    setenv LD_64CB_LIBRARY_PATH "$ld_library_path"
+  }
   if { $orig_dyld_library_path_saved } {
     setenv DYLD_LIBRARY_PATH "$ld_library_path:$orig_dyld_library_path"
   } else {
@@ -199,6 +241,9 @@ proc restore_ld_library_path_env_vars { } {
   global orig_ld_library64_path_saved
   global orig_ld_library_path_32_saved
   global orig_ld_library_path_64_saved
+  global orig_ld_64_library_path_saved
+  global orig_ld_64c_library_path_saved
+  global orig_ld_64cb_library_path_saved
   global orig_dyld_library_path_saved
   global orig_path_saved
   global orig_ld_library_path
@@ -208,6 +253,9 @@ proc restore_ld_library_path_env_vars { } {
   global orig_ld_library64_path
   global orig_ld_library_path_32
   global orig_ld_library_path_64
+  global orig_ld_64_library_path
+  global orig_ld_64c_library_path
+  global orig_ld_64cb_library_path
   global orig_dyld_library_path
   global orig_path
 
@@ -250,6 +298,21 @@ proc restore_ld_library_path_env_vars { } {
   } elseif [info exists env(LD_LIBRARY_PATH_64)] {
     unsetenv LD_LIBRARY_PATH_64
   }
+  if { $orig_ld_64_library_path_saved } {
+    setenv LD_64_LIBRARY_PATH "$orig_ld_64_library_path"
+  } elseif [info exists env(LD_64_LIBRARY_PATH)] {
+    unsetenv LD_64_LIBRARY_PATH
+  }
+  if { $orig_ld_64c_library_path_saved } {
+    setenv LD_64C_LIBRARY_PATH "$orig_ld_64c_library_path"
+  } elseif [info exists env(LD_64C_LIBRARY_PATH)] {
+    unsetenv LD_64C_LIBRARY_PATH
+  }
+  if { $orig_ld_64cb_library_path_saved } {
+    setenv LD_64CB_LIBRARY_PATH "$orig_ld_64cb_library_path"
+  } elseif [info exists env(LD_64CB_LIBRARY_PATH)] {
+    unsetenv LD_64CB_LIBRARY_PATH
+  }
   if { $orig_dyld_library_path_saved } {
     setenv DYLD_LIBRARY_PATH "$orig_dyld_library_path"
   } elseif [info exists env(DYLD_LIBRARY_PATH)] {
diff --git testsuite/libffi.bhaible/test-call.c testsuite/libffi.bhaible/test-call.c
index 4887e00..4d8b4f0 100644
--- testsuite/libffi.bhaible/test-call.c
+++ testsuite/libffi.bhaible/test-call.c
@@ -456,7 +456,11 @@ void
 
 #if (!defined(DGTEST)) || DGTEST == 19
   vpr = vp_vpdpcpsp(&uc1,&d2,str3,&I4);
+#ifdef __CHERI_PURE_CAPABILITY__
+  fprintf(out,"->0x%#p\n",vpr);
+#else
   fprintf(out,"->0x%p\n",vpr);
+#endif
   fflush(out);
   vpr = 0; clear_traces();
   {
@@ -472,7 +476,11 @@ void
       FFI_CALL(cif,vp_vpdpcpsp,args,&vpr);
     }
   }
+#ifdef __CHERI_PURE_CAPABILITY__
+  fprintf(out,"->0x%#p\n",vpr);
+#else
   fprintf(out,"->0x%p\n",vpr);
+#endif
   fflush(out);
 #endif  
   return;
diff --git testsuite/libffi.bhaible/testcases.c testsuite/libffi.bhaible/testcases.c
index 23a6f46..48525e5 100644
--- testsuite/libffi.bhaible/testcases.c
+++ testsuite/libffi.bhaible/testcases.c
@@ -269,7 +269,11 @@ double ABI_ATTR d_d16 (double a, double b, double c, double d, double e, double
 void* ABI_ATTR vp_vpdpcpsp (void* a, double* b, char* c, Int* d)
 {
   void* ret = (char*)b + 1;
-  fprintf(out,"void* f(void*,double*,char*,Int*):(0x%p,0x%p,0x%p,0x%p)",a,b,c,d);
+#ifdef __CHERI_PURE_CAPABILITY__
+  fprintf(out,"void* f(void*,double*,char*,Int*):(%#p,%#p,%#p,%#p)",a,b,c,d);
+#else
+  fprintf(out,"void* f(void*,double*,char*,Int*):(%p,%p,%p,%p)",a,b,c,d);
+#endif
   fflush(out);
   return ret;
 }
